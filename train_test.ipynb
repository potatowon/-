{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c426e2f48fed48dc88007fbad482dc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cbb2fcf4ea405cad4fe98201343a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/682k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237b2a54c4df4d88969b9220ebc89a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8401c615dbe149918ce8b08e3644c3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcb5700f3a64bd49a7684f39835798e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'1일 0 9시까지 최소 20만3220명이 코로나19에 신규 확진되어 역대 최다 기록을 갈아치웠다.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')\n",
    "\n",
    "text = \"\"\"\n",
    "1일 오후 9시까지 최소 20만3220명이 코로나19에 신규 확진됐다. 또다시 동시간대 최다 기록으로, 사상 처음 20만명대에 진입했다.\n",
    "방역 당국과 서울시 등 각 지방자치단체에 따르면 이날 0시부터 오후 9시까지 전국 신규 확진자는 총 20만3220명으로 집계됐다.\n",
    "국내 신규 확진자 수가 20만명대를 넘어선 것은 이번이 처음이다.\n",
    "동시간대 최다 기록은 지난 23일 오후 9시 기준 16만1389명이었는데, 이를 무려 4만1831명이나 웃돌았다. 전날 같은 시간 기록한 13만3481명보다도 6만9739명 많다.\n",
    "확진자 폭증은 3시간 전인 오후 6시 집계에서도 예견됐다.\n",
    "오후 6시까지 최소 17만8603명이 신규 확진돼 동시간대 최다 기록(24일 13만8419명)을 갈아치운 데 이어 이미 직전 0시 기준 역대 최다 기록도 넘어섰다. 역대 최다 기록은 지난 23일 0시 기준 17만1451명이었다.\n",
    "17개 지자체별로 보면 서울 4만6938명, 경기 6만7322명, 인천 1만985명 등 수도권이 12만5245명으로 전체의 61.6%를 차지했다. 서울과 경기는 모두 동시간대 기준 최다로, 처음으로 각각 4만명과 6만명을 넘어섰다.\n",
    "비수도권에서는 7만7975명(38.3%)이 발생했다. 제주를 제외한 나머지 지역에서 모두 동시간대 최다를 새로 썼다.\n",
    "부산 1만890명, 경남 9909명, 대구 6900명, 경북 6977명, 충남 5900명, 대전 5292명, 전북 5150명, 울산 5141명, 광주 5130명, 전남 4996명, 강원 4932명, 충북 3845명, 제주 1513명, 세종 1400명이다.\n",
    "집계를 마감하는 자정까지 시간이 남아있는 만큼 2일 0시 기준으로 발표될 신규 확진자 수는 이보다 더 늘어날 수 있다. 이에 따라 최종 집계되는 확진자 수는 21만명 안팎을 기록할 수 있을 전망이다.\n",
    "한편 전날 하루 선별진료소에서 이뤄진 검사는 70만8763건으로 검사 양성률은 40.5%다. 양성률이 40%를 넘은 것은 이번이 처음이다. 확산세가 계속 거세질 수 있다는 얘기다.\n",
    "이날 0시 기준 신규 확진자는 13만8993명이었다. 이틀 연속 13만명대를 이어갔다.\n",
    "\"\"\"\n",
    "\n",
    "text = text.replace('\\n', ' ')\n",
    "\n",
    "raw_input_ids = tokenizer.encode(text)\n",
    "input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "summary_ids = model.generate(torch.tensor([input_ids]),  num_beams=4,  max_length=512,  eos_token_id=1)\n",
    "tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "# '1일 0 9시까지 최소 20만3220명이 코로나19에 신규 확진되어 역대 최다 기록을 갈아치웠다.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "import lightning as L\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "class KoBARTConditionalGeneration(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams,\n",
    "        **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v1')\n",
    "        self.model.train()\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "        \n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        \n",
    "        self.outputs = defaultdict(list)\n",
    "\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        # Prepare optimizer\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.lr, correct_bias=False)\n",
    "        num_workers = self.hparams.num_workers\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(self.trainer.estimated_stepping_batches * 0.1),\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "\n",
    "        lr_scheduler = {'scheduler': scheduler, \n",
    "                        'monitor': 'loss', 'interval': 'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        attention_mask = inputs['input_ids'].ne(self.pad_token_id).float()\n",
    "        decoder_attention_mask = inputs['decoder_input_ids'].ne(self.pad_token_id).float()\n",
    "        \n",
    "        return self.model(input_ids=inputs['input_ids'],\n",
    "                          attention_mask=attention_mask,\n",
    "                          decoder_input_ids=inputs['decoder_input_ids'],\n",
    "                          decoder_attention_mask=decoder_attention_mask,\n",
    "                          labels=inputs['labels'], return_dict=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outs = self(batch)\n",
    "        loss = outs.loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        outs = self(batch)\n",
    "        loss = outs['loss']\n",
    "        self.outputs[dataloader_idx].append({\"loss\": loss})\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        flat_outputs = []\n",
    "        for lst in self.outputs.values():\n",
    "            flat_outputs.extend(lst)\n",
    "        loss = torch.stack([x[\"loss\"] for x in flat_outputs]).mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.outputs.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning as L\n",
    "from functools import partial\n",
    "\n",
    "class KoBARTSummaryDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer, max_len, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.docs = pd.read_csv(file)\n",
    "        self.len = self.docs.shape[0]\n",
    "\n",
    "        self.pad_index = self.tokenizer.pad_token_id\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def add_padding_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.pad_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def add_ignored_data(self, inputs):\n",
    "        if len(inputs) < self.max_len:\n",
    "            pad = np.array([self.ignore_index] *(self.max_len - len(inputs)))\n",
    "            inputs = np.concatenate([inputs, pad])\n",
    "        else:\n",
    "            inputs = inputs[:self.max_len]\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.docs.iloc[idx]\n",
    "        input_ids = self.tokenizer.encode(instance['news'])\n",
    "        input_ids = self.add_padding_data(input_ids)\n",
    "\n",
    "        label_ids = self.tokenizer.encode(instance['summary'])\n",
    "        label_ids.append(self.tokenizer.eos_token_id)\n",
    "        dec_input_ids = [self.tokenizer.eos_token_id]\n",
    "        dec_input_ids += label_ids[:-1]\n",
    "        dec_input_ids = self.add_padding_data(dec_input_ids)\n",
    "        label_ids = self.add_ignored_data(label_ids)\n",
    "\n",
    "        return {'input_ids': np.array(input_ids, dtype=np.int_),\n",
    "                'decoder_input_ids': np.array(dec_input_ids, dtype=np.int_),\n",
    "                'labels': np.array(label_ids, dtype=np.int_)\n",
    "               }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class KobartSummaryModule(L.LightningDataModule):\n",
    "    def __init__(self, train_file,\n",
    "                 test_file, tok,\n",
    "                 max_len=512,\n",
    "                 batch_size=8,\n",
    "                 num_workers=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.train_file_path = train_file\n",
    "        self.test_file_path =\"/Users/kim-yewon/bigkindsnews/test_data.csv\"\n",
    "        self.tok = tok\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--num_workers',\n",
    "                            type=int,\n",
    "                            default=4,\n",
    "                            help='num of worker for dataloader')\n",
    "        return parser\n",
    "\n",
    "    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n",
    "    def setup(self, stage=None):\n",
    "            # fit 단계에는 훈련 데이터셋을 로드\n",
    "            if stage == 'fit' or stage is None:\n",
    "                self.train = KoBARTSummaryDataset(self.train_file_path, self.tok, self.max_len)\n",
    "                self.val = self.train  # 검증 데이터셋이 별도로 없다면 훈련 데이터셋을 사용\n",
    "\n",
    "            # test 단계에는 테스트 데이터셋을 로드\n",
    "            if stage == 'test' or stage is None:\n",
    "                self.test = KoBARTSummaryDataset(self.test_file_path, self.tok, self.max_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train = DataLoader(self.train,\n",
    "                           batch_size=self.batch_size,\n",
    "                           num_workers=self.num_workers, shuffle=True)\n",
    "        return train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val = DataLoader(self.test,\n",
    "                         batch_size=self.batch_size,\n",
    "                         num_workers=self.num_workers, shuffle=False)\n",
    "        return val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test = DataLoader(self.test,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, shuffle=False)\n",
    "        return test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/kim-yewon/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/kim-yewon/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/kim-yewon/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 123 M \n",
      "-------------------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "495.440   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f85e36d1894ffba2ec7f99508f72b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'KobartSummaryModule' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/kim-yewon/bigkindsnews/KoBART-transepose/KoBART-summarization/train_test.ipynb 셀 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kim-yewon/bigkindsnews/KoBART-transepose/KoBART-summarization/train_test.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# wandb_logger = WandbLogger(project=\"KoBART-summ\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kim-yewon/bigkindsnews/KoBART-transepose/KoBART-summarization/train_test.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m trainer \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39mmax_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kim-yewon/bigkindsnews/KoBART-transepose/KoBART-summarization/train_test.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m                     accelerator\u001b[39m=\u001b[39maccelerator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kim-yewon/bigkindsnews/KoBART-transepose/KoBART-summarization/train_test.ipynb#W6sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m                     devices\u001b[39m=\u001b[39mnum_gpus,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kim-yewon/bigkindsnews/KoBART-transepose/KoBART-summarization/train_test.ipynb#W6sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m                     \u001b[39m# logger=wandb_logger\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kim-yewon/bigkindsnews/KoBART-transepose/KoBART-summarization/train_test.ipynb#W6sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m                     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kim-yewon/bigkindsnews/KoBART-transepose/KoBART-summarization/train_test.ipynb#W6sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, dm)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    534\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1021\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1020\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1021\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1023\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1050\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1047\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1052\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:98\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@_no_grad_context\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[_OUT_DICT]:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_data()\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip:\n\u001b[1;32m    100\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:150\u001b[0m, in \u001b[0;36m_EvaluationLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m stage \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m source \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_source\n\u001b[0;32m--> 150\u001b[0m dataloaders \u001b[39m=\u001b[39m _request_dataloader(source)\n\u001b[1;32m    151\u001b[0m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mstage\u001b[39m.\u001b[39mdataloader_prefix\u001b[39m}\u001b[39;00m\u001b[39m_dataloader()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataloaders, CombinedLoader):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:336\u001b[0m, in \u001b[0;36m_request_dataloader\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \n\u001b[1;32m    327\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m    The requested dataloader\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[1;32m    332\u001b[0m     \u001b[39m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[39m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[39m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m data_source\u001b[39m.\u001b[39;49mdataloader()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:303\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance, pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[1;32m    302\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance\u001b[39m.\u001b[39mtrainer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_lightning_datamodule_hook(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance\u001b[39m.\u001b[39;49mtrainer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m    304\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38_django_42/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:166\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[1;32m    165\u001b[0m     \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningDataModule]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mdatamodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 166\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/bigkindsnews/KoBART-transepose/KoBART-summarization/dataset.py:105\u001b[0m, in \u001b[0;36mKobartSummaryModule.val_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mval_dataloader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 105\u001b[0m     val \u001b[39m=\u001b[39m DataLoader(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest,\n\u001b[1;32m    106\u001b[0m                      batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m    107\u001b[0m                      num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m val\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KobartSummaryModule' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from dataset import KobartSummaryModule\n",
    "from model import KoBARTConditionalGeneration\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "\n",
    "\n",
    "train_file = \"/Users/kim-yewon/bigkindsnews/pair_news.csv\"\n",
    "test_file = \"/Users/kim-yewon/bigkindsnews/test_data.csv\"\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "num_workers = 1\n",
    "\n",
    "max_epochs = 10\n",
    "accelerator = 'cpu'\n",
    "num_gpus = 1\n",
    "gradient_clip_val = 1.0\n",
    "checkpoint = 'checkpoint'\n",
    "\n",
    "\n",
    "# Namespace 객체 생성\n",
    "args = argparse.Namespace(\n",
    "    train_file=train_file,\n",
    "    test_file=test_file,\n",
    "    batch_size=batch_size,\n",
    "    max_len=max_len,\n",
    "    num_workers=num_workers,\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=accelerator,\n",
    "    num_gpus=num_gpus,\n",
    "    gradient_clip_val=gradient_clip_val,\n",
    "    checkpoint=checkpoint,\n",
    "    lr=3e-5\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "dm = KobartSummaryModule(train_file,\n",
    "                    test_file,\n",
    "                    tokenizer,\n",
    "                    batch_size=batch_size,\n",
    "                    max_len=max_len,\n",
    "                    num_workers=num_workers)\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "model = KoBARTConditionalGeneration(args)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n",
    "                                        dirpath=checkpoint,\n",
    "                                        filename='model_chp/{epoch:02d}-{val_loss:.3f}',\n",
    "                                        verbose=True,\n",
    "                                        save_last=True,\n",
    "                                        mode='min',\n",
    "                                        save_top_k=3)\n",
    "\n",
    "# wandb_logger = WandbLogger(project=\"KoBART-summ\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=max_epochs,\n",
    "                    accelerator=accelerator,\n",
    "                    devices=num_gpus,\n",
    "                    gradient_clip_val=gradient_clip_val,\n",
    "                    callbacks=[checkpoint_callback],\n",
    "                    # logger=wandb_logger\n",
    "                    )\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "# dm.setup('test')\n",
    "\n",
    "# # test\n",
    "# trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_django_42",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
